{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4., 3.], dtype=torch.float16) torch.float16\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "a=torch.tensor([4,3], dtype=torch.float16)\n",
    "print(a,a.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.5945, 2.2458, 1.9470, 2.1886],\n",
      "        [2.5945, 2.2458, 1.9470, 2.1886],\n",
      "        [2.5945, 2.2458, 1.9470, 2.1886],\n",
      "        [2.5945, 2.2458, 1.9470, 2.1886],\n",
      "        [2.5945, 2.2458, 1.9470, 2.1886]])\n",
      "tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]]) torch.float32\n"
     ]
    }
   ],
   "source": [
    "b=torch.ones(5,4)\n",
    "c=torch.rand(4,4)\n",
    "print(b@c)\n",
    "print(b,b.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.3000, 4.0000, 2.0000])\n"
     ]
    }
   ],
   "source": [
    "print(torch.Tensor([1.3,4,2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In CPU tensors share same memory location so if we change one variable associated with other, the other one also changes.\n",
    "We can use .clone() to avoid having same memory location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 4., 21.], dtype=torch.float16)\n",
      "tensor([ 4., 21.], dtype=torch.float16)\n",
      "tensor([ 4., 23.], dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "b=a.view(-1)\n",
    "c=a.clone()\n",
    "b[1]=21\n",
    "c[1]=23\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Gradients`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.8969,  0.0696, -0.5814], requires_grad=True)\n",
      "tensor([ 7.2150e-01,  3.3753e-04, -1.9653e-01], grad_fn=<PowBackward0>)\n",
      "tensor([ 1.2421e+00,  3.3765e-04, -1.5791e-01], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x=torch.randn(3,requires_grad=True)\n",
    "y=x**3\n",
    "print(x)\n",
    "print(y)\n",
    "z=y**2+y\n",
    "print(z)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3615, grad_fn=<MeanBackward0>)\n",
      "tensor([1.9652, 0.0049, 0.2052])\n"
     ]
    }
   ],
   "source": [
    "z=z.mean()\n",
    "print(z)\n",
    "z.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We Need to update the grad to 0 after every epoch we use optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "a=torch.randn(3,requires_grad=True)\n",
    "b=a.detach()\n",
    "print(a.requires_grad)\n",
    "print(b.requires_grad)\n",
    "b.requires_grad_(True)\n",
    "print(b.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "a=torch.randn(3,4 ,requires_grad=True)\n",
    "print(a.requires_grad)\n",
    "with torch.no_grad():\n",
    "    b=a+3\n",
    "    print(b.requires_grad)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.5387, -0.5387],\n",
      "        [ 2.3877,  2.3877]])\n",
      "tensor([[-0.5387, -0.5387],\n",
      "        [ 2.3877,  2.3877]])\n"
     ]
    }
   ],
   "source": [
    "v=torch.randn(2,2)\n",
    "k=torch.ones(2,2)\n",
    "print(v@k)\n",
    "print(torch.mm(v,k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating grad for y=x*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10 : w = 3.020 , loss = 0.004\n",
      "epoch 20 : w = 3.021 , loss = 0.003\n",
      "epoch 30 : w = 3.020 , loss = 0.003\n",
      "epoch 40 : w = 3.019 , loss = 0.002\n",
      "epoch 50 : w = 3.018 , loss = 0.002\n",
      "epoch 60 : w = 3.018 , loss = 0.002\n",
      "epoch 70 : w = 3.017 , loss = 0.002\n",
      "epoch 80 : w = 3.016 , loss = 0.002\n",
      "epoch 90 : w = 3.016 , loss = 0.002\n",
      "epoch 100 : w = 3.015 , loss = 0.001\n",
      "Prediction after training tensor([[10.]]) = 30.066\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "X=torch.tensor([[1],[2],[3],[4],[5],[6],[7],[8]],dtype=torch.float32)\n",
    "Y=torch.tensor([[3],[6],[9],[12],[15],[18],[21],[24]],dtype=torch.float32)\n",
    "\n",
    "n_samples,n_features=X.shape\n",
    "input_size=n_features\n",
    "output_size=n_features\n",
    "\n",
    "class linearRegression(nn.Module):\n",
    "    def __init__(self,input_dim,output_dim):\n",
    "        super(linearRegression,self).__init__()\n",
    "        self.lin=nn.Linear(input_dim,output_dim)\n",
    "    def forward(self,x):\n",
    "        return self.lin(x)\n",
    "\n",
    "model=linearRegression(input_size,output_size)\n",
    "learning_rate=0.01\n",
    "n_epochs=100\n",
    "criterion=nn.MSELoss()\n",
    "optimizer=torch.optim.SGD(model.parameters(),lr=learning_rate)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    y_pred=model(X)\n",
    "\n",
    "    loss=criterion(Y,y_pred)\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "   \n",
    "    if (epoch+1)%10==0:\n",
    "        w,b=model.parameters()\n",
    "        print(f\"epoch {epoch+1} : w = {w[0][0].item():.3f} , loss = {loss.item():.3f}\")\n",
    "    \n",
    "\n",
    "X_test=torch.tensor([[10]],dtype=torch.float32)\n",
    "\n",
    "print(f'Prediction after training {X_test} = {model(X_test).item():.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Linear Regression in PyTorch*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 100 : w = 65.574 , loss = 563.803\n",
      "epoch 200 : w = 78.981 , loss = 342.565\n",
      "epoch 300 : w = 81.753 , loss = 333.009\n",
      "epoch 400 : w = 82.331 , loss = 332.587\n",
      "epoch 500 : w = 82.452 , loss = 332.568\n",
      "epoch 600 : w = 82.478 , loss = 332.568\n",
      "epoch 700 : w = 82.483 , loss = 332.568\n",
      "epoch 800 : w = 82.484 , loss = 332.568\n",
      "epoch 900 : w = 82.484 , loss = 332.568\n",
      "epoch 1000 : w = 82.484 , loss = 332.568\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "#Dataset init\n",
    "X_numpy,y_numpy=datasets.make_regression(n_samples=100,n_features=1,noise=20,random_state=1)\n",
    "X=torch.from_numpy(X_numpy.astype(np.float32))\n",
    "y=torch.from_numpy(y_numpy.astype(np.float32))\n",
    "y=y.view(y.shape[0],1)\n",
    "\n",
    "n_samples,n_features=X.shape\n",
    "input_size=n_features\n",
    "output_size=n_features\n",
    "\n",
    "#Model\n",
    "class LinearReg(nn.Module):\n",
    "    def __init__(self,input_dim,output_dim ):\n",
    "        super(LinearReg,self).__init__()\n",
    "        self.lin=nn.Linear(input_dim,output_dim)\n",
    "    def forward(self,X):\n",
    "        return self.lin(X)\n",
    "    \n",
    "model=LinearReg(input_size,output_size)\n",
    "\n",
    "#Training\n",
    "learning_rate=0.01\n",
    "n_epochs=1000\n",
    "criterion=nn.MSELoss()\n",
    "optimizer=torch.optim.SGD(model.parameters(),lr=learning_rate)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    y_pred=model(X)\n",
    "\n",
    "    loss=criterion(y_pred,y)\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if((epoch+1)%100==0):\n",
    "        w,b=model.parameters()\n",
    "        print(f'epoch {epoch+1} : w = {w[0][0]:.3f} , loss = {loss.item():.3f}')\n",
    "\n",
    "\n",
    "#Test\n",
    "\n",
    "predicted=model(X).detach().numpy()\n",
    "plt.plot(X_numpy,y_numpy,'ro')\n",
    "plt.plot(X_numpy,predicted,'b')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Logistic Regression in PyTorch*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 100 : w = -0.381 , loss = 0.115\n",
      "epoch 200 : w = -0.458 , loss = 0.087\n",
      "epoch 300 : w = -0.503 , loss = 0.075\n",
      "epoch 400 : w = -0.535 , loss = 0.068\n",
      "epoch 500 : w = -0.560 , loss = 0.063\n",
      "epoch 600 : w = -0.580 , loss = 0.059\n",
      "epoch 700 : w = -0.597 , loss = 0.056\n",
      "epoch 800 : w = -0.612 , loss = 0.054\n",
      "epoch 900 : w = -0.626 , loss = 0.052\n",
      "epoch 1000 : w = -0.638 , loss = 0.050\n",
      "Accuracy 96.491 %\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "bc= datasets.load_breast_cancer()\n",
    "X,y= bc.data , bc.target\n",
    "\n",
    "n_samples,n_features= X.shape\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y, test_size=0.20 , random_state=1234)\n",
    "\n",
    "\n",
    "#Scaling Features (Always scale for logistic regression)\n",
    "\n",
    "sc= StandardScaler()\n",
    "X_train=sc.fit_transform(X_train)\n",
    "X_test=sc.transform(X_test)\n",
    "\n",
    "X_train=torch.from_numpy(X_train.astype(np.float32))\n",
    "X_test=torch.from_numpy(X_test.astype(np.float32))\n",
    "y_train=torch.from_numpy(y_train.astype(np.float32))\n",
    "y_test=torch.from_numpy(y_test.astype(np.float32))\n",
    "\n",
    "y_train=y_train.view(y_train.shape[0],1)\n",
    "y_test=y_test.view(y_test.shape[0],1)\n",
    "\n",
    "#Model\n",
    "class LogisticReg(nn.Module):\n",
    "    def __init__(self,n_input_features):\n",
    "        super(LogisticReg,self).__init__()\n",
    "        self.lin=nn.Linear(n_input_features,1)\n",
    "\n",
    "    def forward(self,x):\n",
    "        return torch.sigmoid(self.lin(x))\n",
    "    \n",
    "model=LogisticReg(n_features)\n",
    "\n",
    "#loss criterion & optimizer\n",
    "learning_rate=0.05\n",
    "criterion=nn.BCELoss()\n",
    "optimizer=torch.optim.SGD(model.parameters(),lr=learning_rate)\n",
    "\n",
    "#Training\n",
    "n_epochs=1000\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    #forward pass\n",
    "    y_pred=model(X_train)\n",
    "\n",
    "    #loss\n",
    "    loss=criterion(y_pred,y_train)\n",
    "\n",
    "    #backprop\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    if((epoch+1)%100==0):\n",
    "        w,b=model.parameters()\n",
    "        print(f'epoch {epoch+1} : w = {w[0][0]:.3f} , loss = {loss.item():.3f}')\n",
    "\n",
    "\n",
    "#Eval\n",
    "with torch.no_grad():\n",
    "    y_predicted= model(X_test)\n",
    "    y_predicted_cls=y_predicted.round()\n",
    "    acc= y_predicted_cls.eq(y_test).sum()/float(y_test.shape[0])\n",
    "    print(f'Accuracy {acc*100:.3f} %')\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Datasets & Dataloader*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2  Step 5/45 Input size: torch.Size([4, 13]) \n",
      "Epoch 1/2  Step 10/45 Input size: torch.Size([4, 13]) \n",
      "Epoch 1/2  Step 15/45 Input size: torch.Size([4, 13]) \n",
      "Epoch 1/2  Step 20/45 Input size: torch.Size([4, 13]) \n",
      "Epoch 1/2  Step 25/45 Input size: torch.Size([4, 13]) \n",
      "Epoch 1/2  Step 30/45 Input size: torch.Size([4, 13]) \n",
      "Epoch 1/2  Step 35/45 Input size: torch.Size([4, 13]) \n",
      "Epoch 1/2  Step 40/45 Input size: torch.Size([4, 13]) \n",
      "Epoch 1/2  Step 45/45 Input size: torch.Size([2, 13]) \n",
      "Epoch 2/2  Step 5/45 Input size: torch.Size([4, 13]) \n",
      "Epoch 2/2  Step 10/45 Input size: torch.Size([4, 13]) \n",
      "Epoch 2/2  Step 15/45 Input size: torch.Size([4, 13]) \n",
      "Epoch 2/2  Step 20/45 Input size: torch.Size([4, 13]) \n",
      "Epoch 2/2  Step 25/45 Input size: torch.Size([4, 13]) \n",
      "Epoch 2/2  Step 30/45 Input size: torch.Size([4, 13]) \n",
      "Epoch 2/2  Step 35/45 Input size: torch.Size([4, 13]) \n",
      "Epoch 2/2  Step 40/45 Input size: torch.Size([4, 13]) \n",
      "Epoch 2/2  Step 45/45 Input size: torch.Size([2, 13]) \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "class WineDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        xy=np.loadtxt('../Datasets/wine.csv',delimiter=',',dtype=np.float32,skiprows=1)\n",
    "        self.x=torch.from_numpy(xy[:,1:])\n",
    "        self.y=torch.from_numpy(xy[:,[0]])\n",
    "        self.n_samples=xy.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "    \n",
    "\n",
    "dataset=WineDataset()\n",
    "dataloader=DataLoader(dataset=dataset,batch_size=4,shuffle=True, num_workers=0)\n",
    "'''\n",
    "dataiter=iter(dataloader)\n",
    "data=next(dataiter)\n",
    "\n",
    "features,labels=data\n",
    "print(features ,labels)\n",
    "'''\n",
    "\n",
    "\n",
    "#Sample Training\n",
    "\n",
    "n_epochs=2\n",
    "batch_size=4\n",
    "total_samples=len(dataset)\n",
    "n_iter=math.ceil(total_samples/batch_size)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for i,(inputs,labels) in enumerate(dataloader):\n",
    "        if (i+1)%5==0:\n",
    "            print(f'Epoch {epoch+1}/{n_epochs}  Step {i+1}/{n_iter} Input size: {inputs.shape} ')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Dataset Transforms*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.4230e+01, 1.7100e+00, 2.4300e+00, 1.5600e+01, 1.2700e+02, 2.8000e+00,\n",
      "        3.0600e+00, 2.8000e-01, 2.2900e+00, 5.6400e+00, 1.0400e+00, 3.9200e+00,\n",
      "        1.0650e+03])\n",
      "tensor([5.6920e+01, 6.8400e+00, 9.7200e+00, 6.2400e+01, 5.0800e+02, 1.1200e+01,\n",
      "        1.2240e+01, 1.1200e+00, 9.1600e+00, 2.2560e+01, 4.1600e+00, 1.5680e+01,\n",
      "        4.2600e+03])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "class WineDataset(Dataset):\n",
    "    def __init__(self,transform):\n",
    "        xy=np.loadtxt('../Datasets/wine.csv',delimiter=',',dtype=np.float32,skiprows=1)\n",
    "        self.x=xy[:,1:]\n",
    "        self.y=xy[:,[0]]\n",
    "        self.n_samples=xy.shape[0]\n",
    "        self.transform=transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample= self.x[index], self.y[index]\n",
    "        if(self.transform):\n",
    "            sample=self.transform(sample)\n",
    "        return sample\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "    \n",
    "class ToTensor:\n",
    "    def __call__(self,sample):\n",
    "        inputs,targets=sample\n",
    "        return torch.from_numpy(inputs), torch.from_numpy(targets)\n",
    "\n",
    "class MulTransform:\n",
    "    def __init__(self,factor):\n",
    "        self.factor=factor\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        inputs,targets=sample\n",
    "        inputs*=self.factor\n",
    "        return inputs,targets\n",
    "        \n",
    "\n",
    "dataset=WineDataset(transform=ToTensor())\n",
    "first_data=dataset[0]\n",
    "features,labels=first_data\n",
    "print(features)\n",
    "\n",
    "\n",
    "composed=torchvision.transforms.Compose([ToTensor(),MulTransform(4)])\n",
    "dataset=WineDataset(transform=composed)\n",
    "fist_data=dataset[0]\n",
    "features,labels=fist_data\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Activation Functions*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,):\n",
    "        super(NeuralNet,self).__init__()\n",
    "        self.linear1=nn.Linear(input_size,hidden_size)\n",
    "        self.relu=nn.ReLU()\n",
    "        self.linear2=nn.Linear(hidden_size,1)\n",
    "        self.sigmoid=nn.Sigmoid()\n",
    "\n",
    "    def forward(self,x):\n",
    "        out=self.linear1(x)\n",
    "        out=self.relu(out)\n",
    "        out=self.linear2(out)\n",
    "        out=self.sigmoid(out)\n",
    "        return out\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*MNIST Dataset Implementation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1/2 Step:5/600 Loss:2.1543\n",
      "Epoch:1/2 Step:10/600 Loss:1.8823\n",
      "Epoch:1/2 Step:15/600 Loss:1.7131\n",
      "Epoch:1/2 Step:20/600 Loss:1.5081\n",
      "Epoch:1/2 Step:25/600 Loss:1.2736\n",
      "Epoch:1/2 Step:30/600 Loss:1.1447\n",
      "Epoch:1/2 Step:35/600 Loss:1.0610\n",
      "Epoch:1/2 Step:40/600 Loss:0.7736\n",
      "Epoch:1/2 Step:45/600 Loss:0.8556\n",
      "Epoch:1/2 Step:50/600 Loss:0.6973\n",
      "Epoch:1/2 Step:55/600 Loss:0.6889\n",
      "Epoch:1/2 Step:60/600 Loss:0.6652\n",
      "Epoch:1/2 Step:65/600 Loss:0.6188\n",
      "Epoch:1/2 Step:70/600 Loss:0.5934\n",
      "Epoch:1/2 Step:75/600 Loss:0.4840\n",
      "Epoch:1/2 Step:80/600 Loss:0.5808\n",
      "Epoch:1/2 Step:85/600 Loss:0.5770\n",
      "Epoch:1/2 Step:90/600 Loss:0.4038\n",
      "Epoch:1/2 Step:95/600 Loss:0.4501\n",
      "Epoch:1/2 Step:100/600 Loss:0.5330\n",
      "Epoch:1/2 Step:105/600 Loss:0.4751\n",
      "Epoch:1/2 Step:110/600 Loss:0.4349\n",
      "Epoch:1/2 Step:115/600 Loss:0.4616\n",
      "Epoch:1/2 Step:120/600 Loss:0.4138\n",
      "Epoch:1/2 Step:125/600 Loss:0.3277\n",
      "Epoch:1/2 Step:130/600 Loss:0.5053\n",
      "Epoch:1/2 Step:135/600 Loss:0.3803\n",
      "Epoch:1/2 Step:140/600 Loss:0.4669\n",
      "Epoch:1/2 Step:145/600 Loss:0.3176\n",
      "Epoch:1/2 Step:150/600 Loss:0.2708\n",
      "Epoch:1/2 Step:155/600 Loss:0.3639\n",
      "Epoch:1/2 Step:160/600 Loss:0.3950\n",
      "Epoch:1/2 Step:165/600 Loss:0.4691\n",
      "Epoch:1/2 Step:170/600 Loss:0.3712\n",
      "Epoch:1/2 Step:175/600 Loss:0.3930\n",
      "Epoch:1/2 Step:180/600 Loss:0.4948\n",
      "Epoch:1/2 Step:185/600 Loss:0.3425\n",
      "Epoch:1/2 Step:190/600 Loss:0.4116\n",
      "Epoch:1/2 Step:195/600 Loss:0.2486\n",
      "Epoch:1/2 Step:200/600 Loss:0.2028\n",
      "Epoch:1/2 Step:205/600 Loss:0.4336\n",
      "Epoch:1/2 Step:210/600 Loss:0.3277\n",
      "Epoch:1/2 Step:215/600 Loss:0.3836\n",
      "Epoch:1/2 Step:220/600 Loss:0.4200\n",
      "Epoch:1/2 Step:225/600 Loss:0.3410\n",
      "Epoch:1/2 Step:230/600 Loss:0.4452\n",
      "Epoch:1/2 Step:235/600 Loss:0.2412\n",
      "Epoch:1/2 Step:240/600 Loss:0.3353\n",
      "Epoch:1/2 Step:245/600 Loss:0.2026\n",
      "Epoch:1/2 Step:250/600 Loss:0.3973\n",
      "Epoch:1/2 Step:255/600 Loss:0.2663\n",
      "Epoch:1/2 Step:260/600 Loss:0.3062\n",
      "Epoch:1/2 Step:265/600 Loss:0.3603\n",
      "Epoch:1/2 Step:270/600 Loss:0.4818\n",
      "Epoch:1/2 Step:275/600 Loss:0.3369\n",
      "Epoch:1/2 Step:280/600 Loss:0.3574\n",
      "Epoch:1/2 Step:285/600 Loss:0.2542\n",
      "Epoch:1/2 Step:290/600 Loss:0.3838\n",
      "Epoch:1/2 Step:295/600 Loss:0.3481\n",
      "Epoch:1/2 Step:300/600 Loss:0.2902\n",
      "Epoch:1/2 Step:305/600 Loss:0.2357\n",
      "Epoch:1/2 Step:310/600 Loss:0.3291\n",
      "Epoch:1/2 Step:315/600 Loss:0.3289\n",
      "Epoch:1/2 Step:320/600 Loss:0.1764\n",
      "Epoch:1/2 Step:325/600 Loss:0.2287\n",
      "Epoch:1/2 Step:330/600 Loss:0.1770\n",
      "Epoch:1/2 Step:335/600 Loss:0.2767\n",
      "Epoch:1/2 Step:340/600 Loss:0.3396\n",
      "Epoch:1/2 Step:345/600 Loss:0.2294\n",
      "Epoch:1/2 Step:350/600 Loss:0.2023\n",
      "Epoch:1/2 Step:355/600 Loss:0.2057\n",
      "Epoch:1/2 Step:360/600 Loss:0.2928\n",
      "Epoch:1/2 Step:365/600 Loss:0.3191\n",
      "Epoch:1/2 Step:370/600 Loss:0.1953\n",
      "Epoch:1/2 Step:375/600 Loss:0.2170\n",
      "Epoch:1/2 Step:380/600 Loss:0.2548\n",
      "Epoch:1/2 Step:385/600 Loss:0.4131\n",
      "Epoch:1/2 Step:390/600 Loss:0.2387\n",
      "Epoch:1/2 Step:395/600 Loss:0.2278\n",
      "Epoch:1/2 Step:400/600 Loss:0.2831\n",
      "Epoch:1/2 Step:405/600 Loss:0.3340\n",
      "Epoch:1/2 Step:410/600 Loss:0.3923\n",
      "Epoch:1/2 Step:415/600 Loss:0.3060\n",
      "Epoch:1/2 Step:420/600 Loss:0.1555\n",
      "Epoch:1/2 Step:425/600 Loss:0.1958\n",
      "Epoch:1/2 Step:430/600 Loss:0.3195\n",
      "Epoch:1/2 Step:435/600 Loss:0.3063\n",
      "Epoch:1/2 Step:440/600 Loss:0.3995\n",
      "Epoch:1/2 Step:445/600 Loss:0.1846\n",
      "Epoch:1/2 Step:450/600 Loss:0.2351\n",
      "Epoch:1/2 Step:455/600 Loss:0.2253\n",
      "Epoch:1/2 Step:460/600 Loss:0.2127\n",
      "Epoch:1/2 Step:465/600 Loss:0.2092\n",
      "Epoch:1/2 Step:470/600 Loss:0.2402\n",
      "Epoch:1/2 Step:475/600 Loss:0.1219\n",
      "Epoch:1/2 Step:480/600 Loss:0.3179\n",
      "Epoch:1/2 Step:485/600 Loss:0.2810\n",
      "Epoch:1/2 Step:490/600 Loss:0.1497\n",
      "Epoch:1/2 Step:495/600 Loss:0.2990\n",
      "Epoch:1/2 Step:500/600 Loss:0.3739\n",
      "Epoch:1/2 Step:505/600 Loss:0.2426\n",
      "Epoch:1/2 Step:510/600 Loss:0.1752\n",
      "Epoch:1/2 Step:515/600 Loss:0.2537\n",
      "Epoch:1/2 Step:520/600 Loss:0.4306\n",
      "Epoch:1/2 Step:525/600 Loss:0.2152\n",
      "Epoch:1/2 Step:530/600 Loss:0.2098\n",
      "Epoch:1/2 Step:535/600 Loss:0.2520\n",
      "Epoch:1/2 Step:540/600 Loss:0.3235\n",
      "Epoch:1/2 Step:545/600 Loss:0.3332\n",
      "Epoch:1/2 Step:550/600 Loss:0.2744\n",
      "Epoch:1/2 Step:555/600 Loss:0.3125\n",
      "Epoch:1/2 Step:560/600 Loss:0.2311\n",
      "Epoch:1/2 Step:565/600 Loss:0.1978\n",
      "Epoch:1/2 Step:570/600 Loss:0.1515\n",
      "Epoch:1/2 Step:575/600 Loss:0.3289\n",
      "Epoch:1/2 Step:580/600 Loss:0.1455\n",
      "Epoch:1/2 Step:585/600 Loss:0.2569\n",
      "Epoch:1/2 Step:590/600 Loss:0.2244\n",
      "Epoch:1/2 Step:595/600 Loss:0.2805\n",
      "Epoch:1/2 Step:600/600 Loss:0.2154\n",
      "Epoch:2/2 Step:5/600 Loss:0.1814\n",
      "Epoch:2/2 Step:10/600 Loss:0.1072\n",
      "Epoch:2/2 Step:15/600 Loss:0.3151\n",
      "Epoch:2/2 Step:20/600 Loss:0.1250\n",
      "Epoch:2/2 Step:25/600 Loss:0.3533\n",
      "Epoch:2/2 Step:30/600 Loss:0.2444\n",
      "Epoch:2/2 Step:35/600 Loss:0.1707\n",
      "Epoch:2/2 Step:40/600 Loss:0.1705\n",
      "Epoch:2/2 Step:45/600 Loss:0.2383\n",
      "Epoch:2/2 Step:50/600 Loss:0.2679\n",
      "Epoch:2/2 Step:55/600 Loss:0.1723\n",
      "Epoch:2/2 Step:60/600 Loss:0.1852\n",
      "Epoch:2/2 Step:65/600 Loss:0.3815\n",
      "Epoch:2/2 Step:70/600 Loss:0.2437\n",
      "Epoch:2/2 Step:75/600 Loss:0.1552\n",
      "Epoch:2/2 Step:80/600 Loss:0.2159\n",
      "Epoch:2/2 Step:85/600 Loss:0.1956\n",
      "Epoch:2/2 Step:90/600 Loss:0.1232\n",
      "Epoch:2/2 Step:95/600 Loss:0.2138\n",
      "Epoch:2/2 Step:100/600 Loss:0.4038\n",
      "Epoch:2/2 Step:105/600 Loss:0.2281\n",
      "Epoch:2/2 Step:110/600 Loss:0.2358\n",
      "Epoch:2/2 Step:115/600 Loss:0.1170\n",
      "Epoch:2/2 Step:120/600 Loss:0.2133\n",
      "Epoch:2/2 Step:125/600 Loss:0.2328\n",
      "Epoch:2/2 Step:130/600 Loss:0.2735\n",
      "Epoch:2/2 Step:135/600 Loss:0.2613\n",
      "Epoch:2/2 Step:140/600 Loss:0.3426\n",
      "Epoch:2/2 Step:145/600 Loss:0.1788\n",
      "Epoch:2/2 Step:150/600 Loss:0.1636\n",
      "Epoch:2/2 Step:155/600 Loss:0.1206\n",
      "Epoch:2/2 Step:160/600 Loss:0.1538\n",
      "Epoch:2/2 Step:165/600 Loss:0.2141\n",
      "Epoch:2/2 Step:170/600 Loss:0.1105\n",
      "Epoch:2/2 Step:175/600 Loss:0.1204\n",
      "Epoch:2/2 Step:180/600 Loss:0.1036\n",
      "Epoch:2/2 Step:185/600 Loss:0.2514\n",
      "Epoch:2/2 Step:190/600 Loss:0.2776\n",
      "Epoch:2/2 Step:195/600 Loss:0.1770\n",
      "Epoch:2/2 Step:200/600 Loss:0.1881\n",
      "Epoch:2/2 Step:205/600 Loss:0.1741\n",
      "Epoch:2/2 Step:210/600 Loss:0.2480\n",
      "Epoch:2/2 Step:215/600 Loss:0.1409\n",
      "Epoch:2/2 Step:220/600 Loss:0.1811\n",
      "Epoch:2/2 Step:225/600 Loss:0.1824\n",
      "Epoch:2/2 Step:230/600 Loss:0.2067\n",
      "Epoch:2/2 Step:235/600 Loss:0.2377\n",
      "Epoch:2/2 Step:240/600 Loss:0.2756\n",
      "Epoch:2/2 Step:245/600 Loss:0.1164\n",
      "Epoch:2/2 Step:250/600 Loss:0.1894\n",
      "Epoch:2/2 Step:255/600 Loss:0.1654\n",
      "Epoch:2/2 Step:260/600 Loss:0.1836\n",
      "Epoch:2/2 Step:265/600 Loss:0.2068\n",
      "Epoch:2/2 Step:270/600 Loss:0.1444\n",
      "Epoch:2/2 Step:275/600 Loss:0.1841\n",
      "Epoch:2/2 Step:280/600 Loss:0.2468\n",
      "Epoch:2/2 Step:285/600 Loss:0.3227\n",
      "Epoch:2/2 Step:290/600 Loss:0.1649\n",
      "Epoch:2/2 Step:295/600 Loss:0.2038\n",
      "Epoch:2/2 Step:300/600 Loss:0.1507\n",
      "Epoch:2/2 Step:305/600 Loss:0.1995\n",
      "Epoch:2/2 Step:310/600 Loss:0.1564\n",
      "Epoch:2/2 Step:315/600 Loss:0.2180\n",
      "Epoch:2/2 Step:320/600 Loss:0.2300\n",
      "Epoch:2/2 Step:325/600 Loss:0.0980\n",
      "Epoch:2/2 Step:330/600 Loss:0.3200\n",
      "Epoch:2/2 Step:335/600 Loss:0.1410\n",
      "Epoch:2/2 Step:340/600 Loss:0.1080\n",
      "Epoch:2/2 Step:345/600 Loss:0.1276\n",
      "Epoch:2/2 Step:350/600 Loss:0.2169\n",
      "Epoch:2/2 Step:355/600 Loss:0.2457\n",
      "Epoch:2/2 Step:360/600 Loss:0.2630\n",
      "Epoch:2/2 Step:365/600 Loss:0.1598\n",
      "Epoch:2/2 Step:370/600 Loss:0.1844\n",
      "Epoch:2/2 Step:375/600 Loss:0.1094\n",
      "Epoch:2/2 Step:380/600 Loss:0.1638\n",
      "Epoch:2/2 Step:385/600 Loss:0.2337\n",
      "Epoch:2/2 Step:390/600 Loss:0.1964\n",
      "Epoch:2/2 Step:395/600 Loss:0.2308\n",
      "Epoch:2/2 Step:400/600 Loss:0.1464\n",
      "Epoch:2/2 Step:405/600 Loss:0.2416\n",
      "Epoch:2/2 Step:410/600 Loss:0.1866\n",
      "Epoch:2/2 Step:415/600 Loss:0.2801\n",
      "Epoch:2/2 Step:420/600 Loss:0.2457\n",
      "Epoch:2/2 Step:425/600 Loss:0.1466\n",
      "Epoch:2/2 Step:430/600 Loss:0.1021\n",
      "Epoch:2/2 Step:435/600 Loss:0.2071\n",
      "Epoch:2/2 Step:440/600 Loss:0.2689\n",
      "Epoch:2/2 Step:445/600 Loss:0.1621\n",
      "Epoch:2/2 Step:450/600 Loss:0.2267\n",
      "Epoch:2/2 Step:455/600 Loss:0.1567\n",
      "Epoch:2/2 Step:460/600 Loss:0.1509\n",
      "Epoch:2/2 Step:465/600 Loss:0.2055\n",
      "Epoch:2/2 Step:470/600 Loss:0.1524\n",
      "Epoch:2/2 Step:475/600 Loss:0.1738\n",
      "Epoch:2/2 Step:480/600 Loss:0.1294\n",
      "Epoch:2/2 Step:485/600 Loss:0.1553\n",
      "Epoch:2/2 Step:490/600 Loss:0.1564\n",
      "Epoch:2/2 Step:495/600 Loss:0.1380\n",
      "Epoch:2/2 Step:500/600 Loss:0.2469\n",
      "Epoch:2/2 Step:505/600 Loss:0.2291\n",
      "Epoch:2/2 Step:510/600 Loss:0.4394\n",
      "Epoch:2/2 Step:515/600 Loss:0.1080\n",
      "Epoch:2/2 Step:520/600 Loss:0.1095\n",
      "Epoch:2/2 Step:525/600 Loss:0.1668\n",
      "Epoch:2/2 Step:530/600 Loss:0.1388\n",
      "Epoch:2/2 Step:535/600 Loss:0.1655\n",
      "Epoch:2/2 Step:540/600 Loss:0.1091\n",
      "Epoch:2/2 Step:545/600 Loss:0.0668\n",
      "Epoch:2/2 Step:550/600 Loss:0.0712\n",
      "Epoch:2/2 Step:555/600 Loss:0.1196\n",
      "Epoch:2/2 Step:560/600 Loss:0.1781\n",
      "Epoch:2/2 Step:565/600 Loss:0.2212\n",
      "Epoch:2/2 Step:570/600 Loss:0.0628\n",
      "Epoch:2/2 Step:575/600 Loss:0.1000\n",
      "Epoch:2/2 Step:580/600 Loss:0.2612\n",
      "Epoch:2/2 Step:585/600 Loss:0.2279\n",
      "Epoch:2/2 Step:590/600 Loss:0.2467\n",
      "Epoch:2/2 Step:595/600 Loss:0.2201\n",
      "Epoch:2/2 Step:600/600 Loss:0.1168\n",
      "Accuracy 95.2200 %\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as tranforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#Hyper-Parameters\n",
    "input_size=784 #28x28\n",
    "hidden_size= 100\n",
    "num_classes=10\n",
    "n_epochs=2\n",
    "batch_size=100\n",
    "learning_rate=0.001\n",
    "\n",
    "\n",
    "#MNIST\n",
    "train_dataset=torchvision.datasets.MNIST(root='./data',train=True,transform=tranforms.ToTensor(),download=True)\n",
    "test_dataset=torchvision.datasets.MNIST(root='./data',train=False,transform=tranforms.ToTensor())\n",
    "\n",
    "train_loader=torch.utils.data.DataLoader(dataset=train_dataset,batch_size=batch_size,shuffle=True)\n",
    "\n",
    "test_loader=torch.utils.data.DataLoader(dataset=test_dataset,batch_size=batch_size,shuffle=False)\n",
    "\n",
    "\n",
    "'''\n",
    "example = iter(train_loader)\n",
    "features, labels = next(example)\n",
    "print(features.shape, labels.shape)\n",
    "\n",
    "\n",
    "for i in range(6):\n",
    "    plt.subplot(2,3,i+1)\n",
    "    plt.imshow(features[i][0],cmap='gray')\n",
    "plt.show()\n",
    "'''\n",
    "\n",
    "\n",
    "#Model\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size,hidden_size,num_classes):\n",
    "        super(NeuralNet,self).__init__()\n",
    "        self.l1=nn.Linear(input_size,hidden_size)\n",
    "        self.relu=nn.ReLU()\n",
    "        self.l2=nn.Linear(hidden_size,num_classes)\n",
    "\n",
    "    def forward(self,x):\n",
    "        out=self.l1(x)\n",
    "        out=self.relu(out)\n",
    "        out=self.l2(out)\n",
    "        return out\n",
    "    \n",
    "model=NeuralNet(input_size,hidden_size,num_classes)\n",
    "\n",
    "#Softmax is automatically applied in cross entropy loss\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
    "\n",
    "#Training\n",
    "\n",
    "n_total_steps=len(train_loader)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for i,(images,labels) in enumerate(train_loader):\n",
    "        images=images.reshape(-1,28*28).to(device)\n",
    "        labels=labels.to(device)\n",
    "\n",
    "        #forwardpass\n",
    "        output=model(images)\n",
    "\n",
    "        #backprop\n",
    "        loss=criterion(output,labels)\n",
    "        loss.backward()\n",
    "\n",
    "        #optimizer\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if(i+1)%5==0:\n",
    "            print(f'Epoch:{epoch+1}/{n_epochs} Step:{i+1}/{n_total_steps} Loss:{loss.item():.4f}')\n",
    "\n",
    "\n",
    "#Testing\n",
    "\n",
    "with torch.no_grad():\n",
    "    n_correct=0\n",
    "    n_samples=0\n",
    "    for images,labels in test_loader:\n",
    "        images=images.reshape(-1,28*28).to(device)\n",
    "        labels=labels.to(device)\n",
    "        output=model(images)\n",
    "\n",
    "        _,predictions=torch.max(output,1)\n",
    "\n",
    "        n_samples+=labels.shape[0]\n",
    "        n_correct+= (predictions==labels).sum().item()\n",
    "\n",
    "    acc=n_correct/n_samples\n",
    "    print(f'Accuracy {acc*100:.4f} %')\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
